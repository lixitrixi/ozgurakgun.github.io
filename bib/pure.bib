@inproceedings{413b9d1324cf4826b5ea1a130eb96159,
  title     = "A framework for constraint based local search using ESSENCE",
  abstract  = "Structured Neighbourhood Search (SNS) is a framework for constraint-based local search for problems expressed in the Essence abstract constraint specification language. The local search explores a structured neighbourhood, where each state in the neighbourhood preserves a high level structural feature of the problem. SNS derives highly structured problem-specific neighbourhoods automatically and directly from the features of the ESSENCE specification of the problem. Hence, neighbourhoods can represent important structural features of the problem, such as partitions of sets, even if that structure is obscured in the low-level input format required by a constraint solver. SNS expresses each neighbourhood as a constrained optimisation problem, which is solved with a constraint solver. We have implemented SNS, together with automatic generation of neighbourhoods for high level structures, and report high quality results for several optimisation problems.",
  author    = "Ozgur Akgun and Attieh, {Saad Wasim A} and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William} and Salamon, {Andr{\'a}s Z.} and Patrick Spracklen and Wetter, {James Patrick}",
  note      = "Funding: UK Engineering & Physical Sciences Research Council (EPSRC) grants EP/P015638/1and EP/P026842/1.",
  year      = "2018",
  month     = "7",
  day       = "13",
  doi       = "10.24963/ijcai.2018/173",
  language  = "English",
  pages     = "1242--1248",
  editor    = "J{\'e}r{\^o}me Lang",
  booktitle = "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence",
  publisher = "International Joint Conferences on Artificial Intelligence",
}


@inproceedings{b170a1edfb03483a860a1e1482829944,
  title     = "Automatic discovery and exploitation of promising subproblems for tabulation",
  abstract  = "The performance of a constraint model can often be improved by converting a subproblem into a single table constraint. In this paper we study heuristics for identifying promising subproblems. We propose a small set of heuristics to identify common cases such as expressions that will propagate weakly. The process of discovering promising subproblems and tabulating them is entirely automated in the tool Savile Row. A cache is implemented to avoid tabulating equivalent subproblems many times. We give a simple algorithm to generate table constraints directly from a constraint expression in Savile Row. We demonstrate good performance on the benchmark problems used in earlier work on tabulation, and also for several new problem classes.",
  author    = "Ozgur Akgun and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William} and Salamon, {Andr{\'a}s Z.}",
  note      = "Funding: EP/P015638/1 and EP/P026842/1. Dr Jefferson holds a Royal Society University Research Fellowship.",
  year      = "2018",
  month     = "6",
  day       = "15",
  language  = "English",
  isbn      = "9783319983332",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
  editor    = "John Hooker",
  booktitle = "Principles and Practice of Constraint Programming",
  address   = "Netherlands",
}


@inproceedings{6eef5285c1a0471ebba55a9179298de8,
  title     = "Automatic generation and selection of streamlined constraint models via Monte Carlo search on a model lattice",
  abstract  = "Streamlined constraint reasoning is the addition of uninferred constraints to a constraint model to reduce the search space, while retaining at least one solution. Previously it has been established that it is possible to generate streamliners automatically from abstract constraint specifications in Essence and that effective combinations of streamliners can allow instances of much larger scale to be solved. A shortcoming of the previous approach was the crude exploration of the power set of all combinations using depth and breadth first search. We present a new approach based on Monte Carlo search over the lattice of streamlined models, which efficiently identifies effective streamliner combinations.",
  author    = "Patrick Spracklen and Ozgur Akgun and Miguel, {Ian James}",
  note      = "Funding: EPSRC EP/P015638/1.",
  year      = "2018",
  month     = "6",
  day       = "15",
  language  = "English",
  isbn      = "9783319983332",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
  editor    = "John Hooker",
  booktitle = "Principles and Practice of Constraint Programming",
  address   = "Netherlands",
}


@inproceedings{53282bbae2084f9ab816728d0a72fd7b,
  title     = "Metamorphic testing of constraint solvers",
  abstract  = "Constraint solvers are complex pieces of software and are notoriously difficult to debug. In large part this is due to the difficulty of pinpointing the source of an error in the vast searches these solvers perform, since the effect of an error may only come to light long after the error is made. In addition, an error does not necessarily lead to the wrong result, further complicating the debugging process. A major source of errors in a constraint solver is the complex constraint propagation algorithms that provide the inference that controls and directs the search. In this paper we show that metamorphic testing is a principled way to test constraint solvers by comparing two different implementations of the same constraint. Specifically, specialised propagators for the constraint are tested against the general purpose table constraint propagator. We report on metamorphic testing of the constraint solver Minion. We demonstrate that the metamorphic testing method is very effective for finding artificial bugs introduced by random code mutation.",
  author    = "Ozgur Akgun and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William}",
  note      = "Funding: EPSRC EP/P015638/1 and EP/P026842/1. Dr Jefferson holds a Royal Society University Research Fellowship.",
  year      = "2018",
  month     = "6",
  day       = "15",
  language  = "English",
  isbn      = "9783319983332",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
  editor    = "John Hooker",
  booktitle = "Principles and Practice of Constraint Programming",
  address   = "Netherlands",
}


@conference{c4f1f1048f82470c9f62be594dd5a3a3,
  title    = "Validating Synthetic Longitudinal Populations for evaluation of Population Data Linkage",
  abstract = "Background’Gold-standard’ data to evaluate linkage algorithms are rare. Synthetic data have the advantage that all the true links are known. In the domain of population reconstruction, the ability to synthesize populations on demand, with varying characteristics, allows a linkage approach to be evaluated across a wide range of data. We have implemented ValiPop, a microsimulation model, for this purpose.ApproachValiPop can create many varied populations based upon sets of desired population statistics, thus allowing linkage algorithms to be evaluated across many populations, rather than across a limited number of real world ’gold-standard’ data sets.Given the potential interactions between different desired population statistics, the creation of a population does not necessarily imply that all desired population statistics have been met. To address this we have developed a statistical approach to validate the adherence of created populations to the desired statistics, using a generalized linear model.This talk will discuss the benefits of synthetic data for data linkage evaluation, the approach to validating created populations, and present the results of some initial linkage experiments using our synthetic data.",
  author   = "Dalton, {Thomas Stanley} and Kirby, {Graham Njal Cameron} and Alan Dearle and Ozgur Akgun and MacKenzie, {Monique Lea}",
  year     = "2018",
  month    = "6",
  day      = "11",
  doi      = "10.23889/ijpds.v3i2.504",
  language = "English",
}


@inproceedings{2bf3e6cc02c540439b29015862135919,
  title     = "Using metric space indexing for complete and efficient record linkage",
  abstract  = "Record linkage is the process of identifying records that refer to the same real-world entities, in situations where entity identifiers are unavailable. Records are linked on the basis of similarity between common attributes, with every pair being classified as a link or non-link depending on their degree of similarity. Record linkage is usually performed in a three-step process: first groups of similar candidate records are identified using indexing, pairs within the same group are then compared in more detail, and finally classified. Even state-of-the-art indexing techniques, such as Locality Sensitive Hashing, have potential drawbacks. They may fail to group together some true matching records with high similarity. Conversely, they may group records with low similarity, leading to high computational overhead. We propose using metric space indexing to perform complete record linkage, which results in a parameter-free record linkage process combining indexing, comparison and classification into a single step delivering complete and efficient record linkage. Our experimental evaluation on real-world datasets from several domains shows that linkage using metric space indexing can yield better quality than current indexing techniques, with similar execution cost, without the need for domain knowledge or trial and error to configure the process.",
  keywords  = "Entity resolution, Data matching, Similarity search, Blocking",
  author    = "Özgür Akgün and Alan Dearle and Kirby, {Graham Njal Cameron} and Peter Christen",
  year      = "2018",
  doi       = "10.1007/978-3-319-93040-4_8",
  language  = "English",
  isbn      = "9783319930398",
  series    = "Lecture Notes in Computer Science (Lecture Notes in Artificial Intelligence)",
  publisher = "Springer",
  pages     = "89--101",
  editor    = "Dinh Phung and Tseng, {Vincent S.} and Geoff Webb and Bao Ho and Mohadeseh Ganji and Lida Rashidi",
  booktitle = "Advances in Knowledge Discovery and Data Mining",
  address   = "Netherlands",
}


@article{8a56ff34e5bc4dada3bcc63d391de55e,
  title     = "Automatically improving constraint models in Savile Row",
  abstract  = "When solving a combinatorial problem using Constraint Programming (CP) or Satisfiability (SAT), modelling and formulation are vital and difficult tasks. Even an expert human may explore many alternatives in modelling a single problem. We make a number of contributions in the automated modelling and reformulation of constraint models. We study a range of automated reformulation techniques, finding combinations of techniques which perform particularly well together. We introduce and describe in detail a new algorithm, X-CSE, to perform Associative-Commutative Common Subexpression Elimination (AC-CSE) in constraint problems, significantly improving existing CSE techniques for associative and commutative operators such as +. We demonstrate that these reformulation techniques can be integrated in a single automated constraint modelling tool, called Savile Row, whose architecture we describe. We use Savile Row as an experimental testbed to evaluate each reformulation on a set of 50 problem classes, with 596 instances in total. Our recommended reformulations are well worthwhile even including overheads, especially on harder instances where solver time dominates. With a SAT solver we observed a geometric mean of 2.15 times speedup compared to a straightforward tailored model without recommended reformulations. Using a CP solver, we obtained a geometric mean of 5.96 times speedup for instances taking over 10 seconds to solve.",
  keywords  = "Common subexpression elimination, Constraint satisfaction, Modelling, Propositional satisfiability, Reformulation",
  author    = "Peter Nightingale and Özgür Akgün and Gent, {Ian P.} and Christopher Jefferson and Ian Miguel and Patrick Spracklen",
  note      = "Authors thank the EPSRC for funding this work through grants EP/H004092/1, EP/K015745/1, EP/M003728/1, and EP/P015638/1. In addition, Dr Jefferson is funded by a Royal Society University Research Fellowship.",
  year      = "2017",
  month     = "10",
  doi       = "10.1016/j.artint.2017.07.001",
  language  = "English",
  volume    = "251",
  pages     = "35--61",
  journal   = "Artificial Intelligence",
  issn      = "0004-3702",
  publisher = "Elsevier",
}


@conference{1b2ed603b33f4709870e2b38480f829d,
  title    = "Learning From Past Links: Understanding the Limits of Linkage Quality",
  abstract = "The Digitising Scotland project aims to link 25 million vital event records from 1850s to 1970s. We aim to develop automatic approaches to probabilistic, similarity based record linkage. Linkage quality depends on the choices of keys and similarity measures. However, until now the effect of these choices has been unclear. We study the theoretical limits of automated linkage by performing a post-linkage analysis on two datasets, one from the Isle of Skye and one from Kilmarnock, previously linked by historical demographers. In these datasets, individuals appear on multiple certificates. The linkage problem involves unifying these occurrences e.g. between births and deaths, known as Entity Resolution. This requires the choice of particular keys, a similarity measure and a threshold signalling equivalence. We calculate linkage quality metrics–precision, recall, and F-measure–for 4 different key combinations, different similarity measures, and a range of threshold values. We present the distribution of similarity values for links and non-links for each configuration and data-set. From these results, we hope to understand the limits of automated probabilistic record linkage. We will use this understanding to inform our approach to the linkage of new unlinked datasets such as the Digitising Scotland dataset. We would welcome the opportunity to apply this approach to other linked demographic datasets.",
  author   = "Ozgur Akgun and Alan Dearle and Eilidh Garrett and Kirby, {Graham Njal Cameron}",
  year     = "2017",
  month    = "9",
  day      = "6",
  language = "English",
  note     = "British Society for Population Studies Annual Conference 2017, BSPS Annual Conference ; Conference date: 06-09-2017 Through 08-09-2017",
  url      = "http://www.lse.ac.uk/socialPolicy/Researchcentresandgroups/BSPS/annualConference/Home.aspx",
}


@conference{b23bf2749f964504a25b98b76298e524,
  title    = "Evaluating record linkage: creating longitudinal synthetic data to provide gold-standard linked data sets",
  abstract = "‘Gold-standard’ data to evaluate linkage algorithms are rare. Synthetic data have the advantage that all the true links are known. In the domain of population reconstruction, the ability to synthesise populations on demand, with varying characteristics, allows a linkage approach to be evaluated across a wide range of data sets.We present a micro-simulation model for generating such synthetic populations, taking as input a set of desired statistical properties. It then outlines how these desired properties are verified in the generated populations, and the intended approach to using generated populations to evaluate linkage algorithms. We envisage a sequence of experiments where a set of populations are generated to consider how linkage quality varies across different populations: with the same characteristics, with differing characteristics, and with differing types and levels of corruption. The performance of an approach at scale is also considered.",
  keywords = "record linkage",
  author   = "Dalton, {Thomas Stanley} and Alan Dearle and Kirby, {Graham Njal Cameron} and Ozgur Akgun",
  year     = "2017",
  month    = "5",
  day      = "11",
  language = "English",
  note     = "Workshop for the Systematic Linking of Historical Records ; Conference date: 11-05-2017 Through 13-05-2017",
  url      = "http://recordlink.org",
}


@conference{0f94ac46dc094dbfbf3e719ec7b8e43c,
  title    = "Probabilistic linkage of vital event records in Scotland using familial groups",
  abstract = "We report on the assembly of longitudinal data from Scottish birth, death and marriage records representing eighteen million individuals. An experimental approach based on familial groups starts by gathering parents and their siblings into bundles with the aim of (as near of possible) partitioning the certificates into familial groups. This may be achieved by bundling marriage and birth certificates according to a signature derived from their attributes. This is similar to but different from blocking used in most entity resolution schemes where certificates of one kind are gathered together. We have experimented with these techniques using hand coded data from an historic Scottish dataset as a gold standard for comparison. In this paper we will report on our techniques and some preliminary results from our experiments.",
  keywords = "record linkage",
  author   = "Ozgur Akgun and Dalton, {Thomas Stanley} and Alan Dearle and Eilidh Garrett and Kirby, {Graham Njal Cameron}",
  year     = "2017",
  month    = "5",
  day      = "11",
  language = "English",
  note     = "Workshop for the Systematic Linking of Historical Records ; Conference date: 11-05-2017 Through 13-05-2017",
  url      = "http://recordlink.org",
}


@conference{421d8687dc4e4d64b23a7b8eeb63f63b,
  title    = "An identifier scheme for the Digitising Scotland project",
  abstract = "The Digitising Scotland project is having the vital records of Scotland transcribed from images of the original handwritten civil registers . Linking the resulting dataset of 24 million vital records covering the lives of 18 million people is a major challenge requiring improved record linkage techniques. Discussions within the multidisciplinary, widely distributed Digitising Scotland project team have been hampered by the teams in each of the institutions using their own identification scheme. To enable fruitful discussions within the Digitising Scotland team, we required a mechanism for uniquely identifying each individual represented on the certificates. From the identifier it should be possible to determine the type of certificate and the role each person played. We have devised a protocol to generate for any individual on the certificate a unique identifier, without using a computer, by exploiting the National Records of Scotland•{\`A}_s registration districts. Importantly, the approach does not rely on the handwritten content of the certificates which reduces the risk of the content being misread resulting in an incorrect identifier. The resulting identifier scheme has improved the internal discussions within the project. This paper discusses the rationale behind the chosen identifier scheme, and presents the format of the different identifiers. The work reported in the paper was supported by the British ESRC under grants ES/K00574X/1(Digitising Scotland) and ES/L007487/1 (Administrative Data Research Center - Scotland).",
  keywords = "record linkage",
  author   = "Ozgur Akgun and Ahmad Al-Sidiqi and Peter Christen and Dalton, {Thomas Stanley} and Alan Dearle and Dibben, {Christopher John Lloyd} and Eilidh Garrett and Alasdair Gray and Kirby, {Graham Njal Cameron} and Alice Reid",
  year     = "2017",
  month    = "4",
  day      = "2",
  language = "English",
  note     = "UK Administrative Data Research Network Annual Research Conference : Social science using administrative data for public benefit, ADRN2017 ; Conference date: 01-06-2017 Through 02-06-2017",
  url      = "http://www.adrn2017.net",
}


@conference{2c922d20ebd541c7a1673a55e379b693,
  title    = "Evaluating population data linkage: assessing stability, scalability, resilience and robustness across many data sets for comprehensive linkage evaluation",
  abstract = "Data linkage approaches are often evaluated with small or few data sets. If a linkage approach is to be used widely, quantifying its performance with varying data sets would be beneficial. In addition, given a data set needs to be linked, the true links are by definition unknown. The success of a linkage approach is thus difficult to comprehensively evaluate. This talk focuses on the use of many synthetic data sets for the evaluation of linkage quality achieved by automatic linkage algorithms in the domain of population reconstruction. It presents an evaluation approach which considers linkage quality when characteristics of the population are varied. We envisage a sequence of experiments where a set of populations are generated to consider how linkage quality varies across different populations: with the same characteristics, with differing characteristics, and with differing types and levels of corruption. The performance of an approach at scale is also considered. The approach to generate synthetic populations with varying characteristics on demand will also be addressed. The use of synthetic populations has the advantage that all the true links are known, thus allowing evaluation as if with real-world 'gold-standard' linked data sets. Given the large number of data sets evaluated against we also give consideration as to how to present these findings. The ability to assess variations in linkage quality across many data sets will assist in the development of new linkage approaches and identifying areas where existing linkage approaches may be more widely applied.",
  keywords = "data linkage",
  author   = "Dalton, {Thomas Stanley} and Ozgur Akgun and Ahmad Al-Sediqi and Peter Christen and Alan Dearle and Eilidh Garrett and Alasdair Gray and Kirby, {Graham Njal Cameron} and Alice Reid",
  year     = "2017",
  month    = "4",
  day      = "2",
  language = "English",
  note     = "UK Administrative Data Research Network Annual Research Conference : Social science using administrative data for public benefit, ADRN2017 ; Conference date: 01-06-2017 Through 02-06-2017",
  url      = "http://www.adrn2017.net",
}


@conference{5caabedd32764dbbbe8c81724c8cd9e2,
  title    = "Record linking using metric space similarity search",
  abstract = "Record linking often employs blocking to reduce the computational complexity of full pairwise comparison. A key is formed from a subset of record attributes. Those records with the same key values are blocked together for detailed comparison. Use of a single blocking key fails to detect many true matches if records contain missing values or errors, since only those records with the same key values are compared. To address missing values, it is common to repeat the matching process using multiple blocking keys, to match records that are identical in a subset of the fields. The presence of erroneous values may be addressed by blocking using key values mapped to a canonical form (e.g. Soundex). However, this does not address other problems such as single digit transcription errors in dates.Blocking is used to categorise records that are candidate matches, in preparation for a pairwise comparison phase which may use various distance metrics, depending on the domain of the values being compared. Each blocking process defines a partition of records. The comparison operations are only applied to pairs of records within the same category.In some contexts, it may be useful to have flexible control over the precision/recall trade-off, depending on the intended use for the matched data, and the degree of conservatism required of the identified links. With blocking, this flexibility is limited by the number of sensible blocking keys that can be identified.In this talk, we describe experiments with a technique based on similarity searching over metric spaces, which appears to offer greater flexibility, and describe some preliminary results using an historic Scottish dataset.",
  keywords = "record linkage",
  author   = "Alan Dearle and Kirby, {Graham Njal Cameron} and Ozgur Akgun and Dalton, {Thomas Stanley}",
  year     = "2017",
  month    = "4",
  day      = "2",
  language = "English",
  note     = "UK Administrative Data Research Network Annual Research Conference : Social science using administrative data for public benefit, ADRN2017 ; Conference date: 01-06-2017 Through 02-06-2017",
  url      = "http://www.adrn2017.net",
}


@article{1a41bce3240a48d8a8c1b977f34215ea,
  title     = "Cloud benchmarking for maximising performance of scientific applications",
  abstract  = "How can applications be deployed on the cloud to achieve maximum performance? This question is challenging to address with the availability of a wide variety of cloud Virtual Machines (VMs) with different performance capabilities. The research reported in this paper addresses the above question by proposing a six step benchmarking methodology in which a user provides a set of weights that indicate how important memory, local communication, computation and storage related operations are to an application. The user can either provide a set of four abstract weights or eight fine grain weights based on the knowledge of the application. The weights along with benchmarking data collected from the cloud are used to generate a set of two rankings - one based only on the performance of the VMs and the other takes both performance and costs into account. The rankings are validated on three case study applications using two validation techniques. The case studies on a set of experimental VMs highlight that maximum performance can be achieved by the three top ranked VMs and maximum performance in a cost-effective manner is achieved by at least one of the top three ranked VMs produced by the methodology.",
  keywords  = "Cloud benchmarking, Cloud performance, Benchmarking methodology, Cloud ranking",
  author    = "Blesson Varghese and Ozgur Akgun and Miguel, {Ian James} and Thai, {Long Thanh} and Barker, {Adam David}",
  note      = "This research was pursued under the EPSRC grant, EP/K015745/1, a Royal Society Industry Fellowship and an AWS Education Research grant.",
  year      = "2016",
  month     = "8",
  day       = "26",
  doi       = "10.1109/TCC.2016.2603476",
  language  = "English",
  volume    = "PP",
  journal   = "IEEE Transactions on Cloud Computing",
  issn      = "2168-7161",
  publisher = "IEEE",
  number    = "99",
}


@inproceedings{03feab994c4544a49e1387c269b809e0,
  title     = "Exploiting short supports for improved encoding of arbitrary constraints into SAT",
  abstract  = "Encoding to SAT and applying a highly efficient modern SAT solver is an increasingly popular method of solving finite-domain constraint problems. In this paper we study encodings of arbitrary constraints where unit propagation on the encoding provides strong reasoning. Specifically, unit propagation on the encoding simulates generalised arc consistency on the original constraint. To create compact and efficient encodings we use the concept of short support. Short support has been successfully applied to create efficient propagation algorithms for arbitrary constraints. A short support of a constraint is similar to a satisfying tuple however a short support is not required to assign every variable in scope. Some variables are left free to take any value. In some cases a short support representation is smaller than the table of satisfying tuples by an exponential factor. We present two encodings based on short supports and evaluate them on a set of benchmark problems, demonstrating a substantial improvement over the state of the art.",
  author    = "Özgür Akgün and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William}",
  year      = "2016",
  doi       = "10.1007/978-3-319-44953-1_1",
  language  = "English",
  isbn      = "9783319449524",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
  pages     = "3--12",
  editor    = "Michael Rueher",
  booktitle = "Principles and Practice of Constraint Programming",
  address   = "Netherlands",
}


@inproceedings{3881455b36054960a6bae0910108ab95,
  title     = "Cloud-based e-Infrastructure for scheduling astronomical observations",
  abstract  = "Gravitational microlensing exploits a transient phenomenon where an observed star is brightened due to deflection of its light by the gravity of an intervening foreground star. It is conjectured that this technique can be used to measurethe abundance of planets throughout the Milky Way. In order to undertake efficient gravitational microlensing an observation schedule must be constructed such that various targets are observed while undergoing a microlensing event. In this paper, we propose a cloud-based e-Infrastructure that currently supportsfour methods to compute candidate schedules via the application of local search and probabilistic meta-heuristics. We then validate the feasibility of the e-Infrastructure by evaluating the methods on historic data. The experiments demonstrate that the use of on-demand cloud resources for the e-Infrastructure can allow better schedules to be found more rapidly.",
  author    = "Wetter, {James Patrick} and Ozgur Akgun and Barker, {Adam David} and Martin Dominik and Miguel, {Ian James} and Blesson Varghese",
  note      = "This research was pursued under the EPSRC grant ‘Working Together: Constraint Programming and Cloud Computing’ (EP/K015745/1) and an Amazon Web Services (AWS) Education Research Grant.",
  year      = "2015",
  month     = "8",
  day       = "31",
  doi       = "10.1109/eScience.2015.54",
  language  = "English",
  pages     = "362--370",
  booktitle = "2015 IEEE 11th International Conference on e-Science (e-Science) (2015)",
  publisher = "IEEE Computer Society",
  address   = "United States",
}


@inproceedings{9dd26cea0c54476f8d0418df430909da,
  title     = "Automatically generating streamlined constraint models with ESSENCE and CONJURE",
  abstract  = "Streamlined constraint reasoning is the addition of uninferred constraints to a constraint model to reduce the search space, while retaining at least one solution. Previously, effective streamlined models have been constructed by hand, requiring an expert to examine closely solutions to small instances of a problem class and identify regularities. We present a system that automatically generates many conjectured regularities for a given Essence specification of a problem class by examining the domains of decision variables present in the problem specification. These conjectures are evaluated independently and in conjunction with one another on a set of instances from the specified class via an automated modelling tool-chain comprising of Conjure, Savile Row and Minion. Once the system has identified effective conjectures they are used to generate streamlined models that allow instances of much larger scale to be solved. Our results demonstrate good models can be identified for problems in combinatorial design, Ramsey theory, graph theory and group theory - often resulting in order of magnitude speed-ups.",
  author    = "James Wetter and Ozgur Akgun and Ian Miguel",
  year      = "2015",
  month     = "8",
  day       = "13",
  doi       = "10.1007/978-3-319-23219-5_34",
  language  = "English",
  isbn      = "9783319232188",
  volume    = "9255",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer",
  pages     = "480--496",
  editor    = "Gilles Pesant",
  booktitle = "Principles and Practice of Constraint Programming",
  address   = "Netherlands",
}


@inproceedings{0e8ec1d427b148acb1f0d20b9ef51659,
  title     = "Cloud benchmarking for performance",
  abstract  = "How can applications be deployed on the cloud to achieve maximum performance? This question has become significant and challenging with the availability of a wide variety of Virtual Machines (VMs) with different performance capabilities in the cloud. The above question is addressed by proposing a six step benchmarking methodology in which a user provides a set of four weights that indicate how important each of the following groups: memory, processor, computation and storage are to the application that needs to be executed on the cloud. The weights along with cloud benchmarking data are used to generate a ranking of VMs that can maximise performance of the application. The rankings are validated through an empirical analysis using two case study applications; the first is a financial risk application and the second is a molecular dynamics simulation, which are both representative of workloads that can benefit from execution on the cloud. Both case studies validate the feasibility of the methodology and highlight that maximum performance can be achieved on the cloud by selecting the top ranked VMs produced by the methodology.",
  author    = "Blesson Varghese and Ozgur Akgun and Ian Miguel and Long Thai and Adam Barker",
  note      = "Date of Acceptance: 20/09/2014",
  year      = "2014",
  month     = "12",
  day       = "15",
  doi       = "10.1109/CloudCom.2014.28",
  language  = "English",
  isbn      = "9781479940936",
  pages     = "535--540",
  booktitle = "6th IEEE International Conference on Cloud Computing Technology and Science (CloudCom 2014)",
  publisher = "IEEE",
}


@inproceedings{09b16cf13ed94fb1b2fed6296b5a75d1,
  title     = "Optimal deployment of geographically distributed workflow engines on the Cloud",
  abstract  = "When orchestrating Web service workflows, the geographical placement of the orchestration engine(s) can greatly affect workflow performance. Data may have to be transferred across long geographical distances, which in turn increases execution time and degrades the overall performance of a workflow. In this paper, we present a framework that, given a DAG-based workflow specification, computes the op- timal Amazon EC2 cloud regions to deploy the orchestration engines and execute a workflow. The framework incorporates a constraint model that solves the workflow deployment problem, which is generated using an automated constraint modelling system. The feasibility of the framework is evaluated by executing different sample workflows representative of sci- entific workloads. The experimental results indicate that the framework reduces the workflow execution time and provides a speed up of 1.3x-2.5x over centralised approaches.",
  keywords  = "Workflow engine, Optimal deployment, Cloud computing, Workflow execution",
  author    = "Long Thai and Adam Barker and Blesson Varghese and Ozgur Akgun and Ian Miguel",
  note      = "This research was pursued under the EPSRC ‘Working Together: Constraint Programming and Cloud Computing’ grant, a Royal Society Industry Fellowship ‘Bringing Science to the Cloud’, and an Amazon Web Services Education Research Grant. Date of Acceptance: 02/09/2014",
  year      = "2014",
  month     = "10",
  day       = "30",
  doi       = "10.1109/CloudCom.2014.30",
  language  = "English",
  isbn      = "9781479940936",
  pages     = "811--816",
  booktitle = "6th IEEE International Conference on Cloud Computing Technology and Science (CloudCom 2014)",
  publisher = "IEEE",
}


@inproceedings{4d61e32b389247fe849054856b224aa1,
  title     = "Automatically Improving Constraint Models in Savile Row through Associative-Commutative Common Subexpression Elimination",
  abstract  = "When solving a problem using constraint programming, constraint modelling is widely acknowledged as an important and difficult task. Even a constraint modelling expert may explore many models and spend considerable time modelling a single problem. Therefore any automated assistance in the area of constraint modelling is valuable. Common sub-expression elimination (CSE) is a type of constraint reformulation that has proved to be useful on a range of problems. In this paper we demonstrate the value of an extension of CSE called Associative-Commutative CSE (AC-CSE). This technique exploits the properties of associativity and commutativity of binary operators, for example in sum constraints. We present a new algorithm, X-CSE, that is able to choose from a larger palette of common subexpressions than previous approaches. We demonstrate substantial gains in performance using X-CSE. For example on BIBD we observed speed increases of more than 20 times compared to a standard model and that using X-CSE outperforms a sophisticated model from the literature. For Killer Sudoku we found that X-CSE can render some apparently difficult instances almost trivial to solve, and we observe speed increases up to 350 times. For BIBD and Killer Sudoku the common subexpressions are not present in the initial model: an important part of our methodology is reformulations at the preprocessing stage, to create the common subexpressions for X-CSE to exploit. In summary we show that X-CSE, combined with preprocessing and other reformulations, is a powerful technique for automated modelling of problems containing associative and commutative constraints.",
  keywords  = "Symmetry, Breaking, System",
  author    = "Peter Nightingale and Ozgur Akgun and Gent, {Ian P.} and Christopher Jefferson and Ian Miguel",
  note      = "We would like to thank the Royal Society for funding through Dr Jefferson’s URF, and the EPSRC for funding this work through grant EP/H004092/1.",
  year      = "2014",
  month     = "9",
  doi       = "10.1007/978-3-319-10428-7_43",
  language  = "English",
  isbn      = "978-3-319-10427-0",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
  pages     = "590--605",
  editor    = "B OSullivan",
  booktitle = "Principles and Practice of Constraint Programming",
  address   = "Netherlands",
}


@inbook{dd9347655a2d45f2bef81ebcb8778daa,
  title     = "Breaking conditional symmetry in automated constraint modelling with CONJURE",
  abstract  = "Many constraint problems contain symmetry, which can lead to redundant search. If a partial assignment is shown to be invalid, we are wasting time if we ever consider a symmetric equivalent of it. A particularly important class of symmetries are those introduced by the constraint modelling process: model symmetries. We present a systematic method by which the automated constraint modelling tool CONJURE can break conditional symmetry as it enters a model during refinement. Our method extends, and is compatible with, our previous work on automated symmetry breaking in CONJURE. The result is the automatic and complete removal of model symmetries for the entire problem class represented by the input specification. This applies to arbitrarily nested conditional symmetries and represents a significant step forward for automated constraint modelling.",
  author    = "Özgür Akgün and Ian Gent and Chris Jefferson and Ian Miguel and Peter Nightingale",
  note      = "This work was supported by UK EPSRC EP/K015745/1. Jefferson is supported by a Royal Society University Research Fellowship.",
  year      = "2014",
  doi       = "10.3233/978-1-61499-419-0-3",
  language  = "English",
  isbn      = "9781614994183",
  volume    = "263",
  series    = "Frontiers in Artificial Intelligence and Applications",
  publisher = "IOS Press",
  pages     = "3--8",
  editor    = "T. Schaub and G. Friedrich and B. O'Sullivan",
  booktitle = "ECAI 2014",
  address   = "Netherlands",
}


@misc{0ef98e79e0bd426eb92227f281f4ee4e,
  title     = "Extensible automated constraint modelling via refinement of abstract problem specifications",
  abstract  = "Constraint Programming (CP) is a powerful technique for solving large-scale combinatorial (optimisation) problems. Constraint solving a given problem proceeds in two phases: modelling and solving. Effective modelling has an huge impact on the performance of the solving process. This thesis presents a framework in which the users are not required to make modelling decisions, concrete CP models are automatically generated from a high level problem specification. In this framework, modelling decisions are encoded as generic rewrite rules applicable to many different problems. First, modelling decisions are divided into two broad categories. This categorisation guides the automation of each kind of modelling decision and also leads us to the architecture of the automated modelling tool. Second, a domain-specific declarative rewrite rule language is introduced. Thanks to the rule language, automated modelling transformations and the core system are decoupled. The rule language greatly increases the extensibility and maintainability of the rewrite rules database. The database of rules represents the modelling knowledge acquired after analysis of expert models. This database must be easily extensible to best benefit from the active research on constraint modelling. Third, the automated modelling system Conjure is implemented as a realisation of these ideas; having an implementation enables empirical testing of the quality of generated models. The ease with which rewrite rules can be encoded to produce good models is shown. Furthermore, thanks to the generality of the system, one needs to add a very small number of rules to encode many transformations. Finally, the work is evaluated by comparing the generated models to expert models found in the literature for a wide variety of benchmark problems. This evaluation confirms the hypothesis that expert models can be automatically generated starting from high level problem specifications. An method of automatically identifying good models is also presented. In summary, this thesis presents a framework to enable the automatic generation of efficient constraint models from problem specifications. It provides a pleasant environment for both problem owners and modelling experts. Problem owners are presented with a fully automated constraint solution process, once they have a precise description of their problem. Modelling experts can now encode their precious modelling expertise as rewrite rules instead of merely modelling a single problem; resulting in reusable constraint modelling knowledge.",
  author    = "Ozgur Akgun",
  year      = "2014",
  language  = "English",
  publisher = "University of St Andrews",
  url       = "http://hdl.handle.net/10023/6547"
}


@inproceedings{ec7a3b357c8b4af4ba335c281ea87ba3,
  title     = "An Automated Constraint Modelling and Solving Toolchain",
  author    = "Ozgur Akgun and Frisch, {Alan M} and Gent, {Ian Philip} and Hussain, {Bilal Syed} and Jefferson, {Christopher Anthony} and Lars Kotthoff and Miguel, {Ian James} and Nightingale, {Peter William}",
  year      = "2013",
  language  = "English",
  booktitle = "ARW 2013 - 20th Automated Reasoning Workshop",
}


@inproceedings{dfc0e4b721844d9d801fb27c264086ff,
  title     = "Automated Modelling and Model Selection in Constraint Programming",
  abstract  = "In attacking the modelling bottleneck, we present current achievements in automated model generation and selection in constraint programming (CP). We also discuss promising future directions in automated model selection, which we believe are of key importance in enabling successful automated modelling in CP.",
  author    = "Ozgur Akgun and Frisch, {Alan M} and Jefferson, {Christopher Anthony} and Miguel, {Ian James}",
  year      = "2013",
  language  = "English",
  booktitle = "COSpeL: The first Workshop on Domain Specific Languages in Combinatorial Optimization",
}


@inproceedings{066dfdbd563a40c68df2eaae83a342cd,
  title     = "Automated Symmetry Breaking and Model Selection in Conjure",
  abstract  = "Constraint modelling is widely recognised as a key bottleneck in applying constraint solving to a problem of interest. The Conjure automated constraint modelling system addresses this problem by automatically refining constraint models from problem specifications written in the Essence language. Essence provides familiar mathematical concepts like sets, functions and relations nested to any depth. To date, Conjure has been able to produce a set of alternative model kernels (i.e. without advanced features such as symmetry breaking or implied constraints) for a given specification. The first contribution of this paper is a method by which Conjure can break symmetry in a model as it is introduced by the modelling process. This works at the problem class level, rather than just individual instances, and does not require an expensive detection step after the model has been formulated. This allows Conjure to produce a higher quality set of models. A further limitation of Conjure has been the lack of a mechanism to select among the models it produces. The second contribution of this paper is to present two such mechanisms, allowing effective models to be chosen automatically.",
  author    = "Ozgur Akgun and Frisch, {Alan M} and Gent, {Ian Philip} and Hussain, {Bilal Syed} and Jefferson, {Christopher Anthony} and Lars Kotthoff and Miguel, {Ian James} and Nightingale, {Peter William}",
  year      = "2013",
  doi       = "10.1007/978-3-642-40627-0_11",
  language  = "English",
  booktitle = "CP 2013 - Principles and Practice of Constraint Programming, 19th International Conference",
}


@inproceedings{ed9a3c921f40425b964217636a5af521,
  title     = "Extensible Automated Constraint Modelling",
  abstract  = "In constraint solving, a critical bottleneck is the formulation of an effective constraint model of a given problem. The CONJURE system described in this paper, a substantial step forward over prototype versions of CONJURE previously reported, makes a valuable contribution to the automation of constraint modelling by automatically producing constraint models from their specifications in the abstract constraint specification language ESSENCE. A set of rules is used to refine an abstract specification into a concrete constraint model. We demonstrate that this set of rules is readily extensible to increase the space of possible constraint models CONJURE can produce. Our empirical results confirm that CONJURE can reproduce successfully the kernels of the constraint models of 32 benchmark problems found in the literature.",
  author    = "Ozgur Akgun and Miguel, {Ian James} and Jefferson, {Christopher Anthony} and Frisch, {Alan M.} and Brahim Hnich",
  year      = "2011",
  language  = "English",
  isbn      = "978-157735508-3",
  pages     = "4--11",
  booktitle = "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence",
  publisher = "AAAI Press",
  URL = "http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3687"
}


@inproceedings{ecb8ca42a8eb4098a685daa84c821da3,
  title     = "The Open Stacks Problem: An automated modelling case study",
  author    = "Ozgur Akgun and Miguel, {Ian James} and Jefferson, {Christopher Anthony}",
  year      = "2011",
  language  = "English",
  pages     = "15",
  booktitle = "ERCIM Workshop on Constraint Solving and Constraint Logic Programming",
  URL = "https://csclp2011.cs.st-andrews.ac.uk/csclp2011proceedings.pdf#page=21"
}


@inbook{aee46a0c83b14b25b687813677855191,
  title     = "Conjure Revisited: Towards Automated Constraint Modelling",
  abstract  = "Automating the constraint modelling process is one of thekey challenges facing the constraints field, and one of the principal obstaclespreventing widespread adoption of constraint solving. This paperfocuses on the refinement-based approach to automated modelling, wherea user specifies a problem in an abstract constraint specification languageand it is then automatically refined into a constraint model. In particular,we revisit the Conjure system that first appeared in prototype formin 2005 and present a new implementation with a much greater coverageof the specification language Essence",
  author    = "Özgür Akgün and Frisch, {Alan M} and Brahim Hnich and Jefferson, {Christopher Anthony} and Miguel, {Ian James}",
  year      = "2010",
  language  = "English",
  booktitle = "ModRef 2010 - The 9th International Workshop on Constraint Modelling and Reformulation",
}


@inproceedings{586c88ee08c1404391bc48c26d66e523,
  title     = "Refining Portfolios of Constraint Models with Conjure",
  abstract  = "Modelling is one of the key challenges in Constraint Programming(CP). There are many ways in which to model a given problem.The model chosen has a substantial effect on the solving efficiency. Itis difficult to know what the best model is. To overcome this problem wetake a portfolio approach: Given a high level specification of a combinatorialproblem, we employ non-deterministic rewrite techniques to obtaina portfolio of constraint models. The specification language (Essence)does not require humans to make modelling decisions; therefore it helpsus remove the modelling bottleneck.",
  author    = "Ozgur Akgun",
  year      = "2010",
  language  = "English",
  pages     = "1--6",
  booktitle = "CP 2010 - Principles and Practice of Constraint Programming, 16th International Conference, Doctoral Program",
}
